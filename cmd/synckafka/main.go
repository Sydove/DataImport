package main

import (
	_ "DataImport/internal/pkg/config"
	"DataImport/internal/pkg/kafka"
	"context"
	"fmt"
	"os"
	"os/signal"
	"sync"
	"syscall"
	"time"

	kafkaGo "github.com/confluentinc/confluent-kafka-go/kafka"
)

func main() {
	// 1. åˆ›å»º Kafka å®¢æˆ·ç«¯
	client, err := kafka.NewClient()
	if err != nil {
		fmt.Printf("âŒ åˆå§‹åŒ– Kafka å®¢æˆ·ç«¯å¤±è´¥: %v\n", err)
		panic(err)
	}
	defer client.Close()

	// 2. è·å–é›†ç¾¤ä¿¡æ¯
	fmt.Println("â•â•â•â•â•â•â•â•â• é›†ç¾¤ä¿¡æ¯ â•â•â•â•â•â•â•â•â•")
	_, err = client.GetClusterMetadata()
	if err != nil {
		fmt.Printf("âŒ è·å–é›†ç¾¤ä¿¡æ¯å¤±è´¥: %v\n", err)
	}

	// 3. åˆ›å»º Topicï¼ˆå¦‚æœéœ€è¦ï¼‰
	// err = client.CreateTopic(kafka.TopicConfig{
	//     Name:              "this_topic",
	//     NumPartitions:     3,
	//     ReplicationFactor: 3,
	// })
	// if err != nil {
	//     fmt.Printf("âŒ åˆ›å»º Topic å¤±è´¥: %v\n", err)
	// }

	wg := sync.WaitGroup{}
	signalChan := make(chan os.Signal, 1)
	signal.Notify(signalChan, syscall.SIGINT, syscall.SIGTERM)

	// 4. å¯åŠ¨ Producer
	wg.Add(1)
	go func() {
		defer wg.Done()
		if err := runProducer(client); err != nil {
			fmt.Printf("âŒ Producer é”™è¯¯: %v\n", err)
		}
	}()

	// 5. å¯åŠ¨å¤šä¸ª Consumer
	for i := 0; i < 3; i++ {
		wg.Add(1)
		go func(consumerID int) {
			defer wg.Done()
			if err := runConsumer(client, consumerID); err != nil {
				fmt.Printf("âŒ Consumer %d é”™è¯¯: %v\n", consumerID, err)
			}
		}(i)
	}

	// 6. ç­‰å¾…é€€å‡ºä¿¡å·
	<-signalChan
	fmt.Println("\nğŸ›‘ æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
}

// runProducer è¿è¡Œ Producer
func runProducer(client *kafka.Client) error {
	// åˆ›å»ºè‡ªå®šä¹‰é…ç½®çš„ Producer
	producer, err := kafka.NewProducer(kafka.ProducerConfig{
		CompressionType: "snappy",
		BatchSize:       51200,
		LingerMs:        10,
		Acks:            "all",
		MaxPending:      10000, // èƒŒå‹é˜ˆå€¼
	})
	if err != nil {
		return err
	}
	defer producer.Close()

	topicName := "this_topic"
	messageCount := 101

	fmt.Printf("ğŸš€ å¼€å§‹å‘é€ %d æ¡æ¶ˆæ¯...\n", messageCount)
	startTime := time.Now()

	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Minute)
	defer cancel()

	wg := sync.WaitGroup{}

	// å‘é€æ¶ˆæ¯
	for i := 0; i <= 100; i++ {
		wg.Add(1)
		go func(i int) {
			defer wg.Done()

			timestamp := time.Now().UnixNano()
			messageValue := fmt.Sprintf("hello world %d", timestamp)

			// ä½¿ç”¨å¸¦èƒŒå‹æ§åˆ¶çš„å‘é€
			err := producer.ProduceWithBackpressure(ctx, &kafkaGo.Message{
				TopicPartition: kafkaGo.TopicPartition{
					Topic:     &topicName,
					Partition: kafkaGo.PartitionAny,
				},
				Value: []byte(messageValue),
			})

			if err != nil {
				fmt.Printf("âš ï¸ å‘é€å¤±è´¥ [msg-%d]: %v\n", i, err)
			}
		}(i)
	}

	wg.Wait()

	submitDuration := time.Since(startTime)
	sent, _, _, backpressureHits := producer.GetStats()

	fmt.Printf("ğŸ“ æ‰€æœ‰æ¶ˆæ¯å·²æäº¤åˆ°é˜Ÿåˆ—ï¼Œå®é™…æäº¤: %d æ¡ï¼Œè€—æ—¶: %vï¼ŒèƒŒå‹è§¦å‘: %d æ¬¡\n",
		sent, submitDuration, backpressureHits)

	// ç­‰å¾…æ‰€æœ‰æ¶ˆæ¯å‘é€å®Œæˆ
	fmt.Println("â³ ç­‰å¾…æ‰€æœ‰æ¶ˆæ¯ç¡®è®¤...")
	waitCtx, waitCancel := context.WithTimeout(context.Background(), 5*time.Minute)
	defer waitCancel()

	if err := producer.WaitForCompletion(waitCtx); err != nil {
		return fmt.Errorf("å‘é€å¤±è´¥: %w", err)
	}

	totalDuration := time.Since(startTime)
	fmt.Printf("ğŸ“ˆ æ€»è€—æ—¶: %v (æäº¤: %v, ç¡®è®¤: %v)\n",
		totalDuration, submitDuration, totalDuration-submitDuration)

	return nil
}

// runConsumer è¿è¡Œ Consumer
func runConsumer(client *kafka.Client, consumerID int) error {
	consumer, err := client.CreateConsumer(kafka.ConsumerConfig{
		GroupID:         "number_one",
		Topics:          []string{"this_topic"},
		AutoOffsetReset: "earliest",
		CommitBatchSize: 10,
	}, consumerID)
	if err != nil {
		return err
	}
	defer consumer.Close()

	// å®šä¹‰æ¶ˆæ¯å¤„ç†å‡½æ•°
	handler := func(msg *kafkaGo.Message) error {
		// è¿™é‡Œå¯ä»¥æ·»åŠ è‡ªå®šä¹‰çš„æ¶ˆæ¯å¤„ç†é€»è¾‘
		// fmt.Printf("å¤„ç†æ¶ˆæ¯: %s\n", string(msg.Value))
		return nil
	}

	// å¼€å§‹æ¶ˆè´¹
	consumer.StartConsuming(consumerID, handler)

	return nil
}
