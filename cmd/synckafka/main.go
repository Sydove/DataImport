package main

import (
	_ "DataImport/internal/pkg/config"
	"context"
	"fmt"
	"os"
	"os/signal"
	"strings"
	"sync"
	"sync/atomic"
	"syscall"
	"time"

	"github.com/confluentinc/confluent-kafka-go/kafka"
	"github.com/spf13/viper"
)

type KafkaClient struct {
	producer    *kafka.Producer
	adminClient *kafka.AdminClient
	consumer    []*kafka.Consumer
}

// ProducerTracker ç”¨äºè·Ÿè¸ª Producer å‘é€çŠ¶æ€
type ProducerTracker struct {
	sentCount       int64 // å·²å‘é€æ¶ˆæ¯æ•°
	successCount    int64 // æˆåŠŸç¡®è®¤æ•°
	failedCount     int64 // å¤±è´¥æ•°
	maxPending      int64 // æœ€å¤§å¾…ç¡®è®¤æ¶ˆæ¯æ•°ï¼ˆèƒŒå‹é˜ˆå€¼ï¼‰
	backpressureHit int64 // èƒŒå‹è§¦å‘æ¬¡æ•°
	mu              sync.Mutex
	failedMsgs      []FailedMessage // å¤±è´¥çš„æ¶ˆæ¯è¯¦æƒ…
}

type FailedMessage struct {
	Value string
	Error error
	Time  time.Time
}

type Message struct {
	ID   string
	Host string
	Port string
}

type ConfigCategory int

const (
	AdminClient ConfigCategory = 1
	Product     ConfigCategory = 2
	Consumer    ConfigCategory = 3
)

func getKafkaConfig(category ConfigCategory) *kafka.ConfigMap {
	kafkaAddress := viper.GetStringSlice("kafka.addr")
	config := &kafka.ConfigMap{
		"bootstrap.servers": strings.Join(kafkaAddress, ","),
	}
	switch category {
	case AdminClient:
		return config
	case Product:
		config.SetKey("compression.type", "snappy")
		config.SetKey("batch.size", 51200)
		config.SetKey("acks", "all")
		config.SetKey("linger.ms", 10)
		config.SetKey("queue.buffering.max.messages", 2000000)
		config.SetKey("queue.buffering.max.kbytes", 1048576)
		config.SetKey("enable.idempotence", true)
	case Consumer:
		config.SetKey("go.events.channel.enable", true)        // å¯ç”¨äº‹ä»¶é€šé“
		config.SetKey("go.application.rebalance.enable", true) // æ¶ˆè´¹è€…ç¦»å¼€é‡åˆ†é…
	}
	return config
}

// NewKafkaClient
//
//	@Description: åˆ›å»ºkafkaå¯¹è±¡ consumer/producer/adminClient
//	@return *KafkaClient
//	@return error
func NewKafkaClient() (*KafkaClient, error) {
	clientConfig := getKafkaConfig(AdminClient)
	admin, err := kafka.NewAdminClient(clientConfig)
	if err != nil {
		return nil, err
	}

	productConfig := getKafkaConfig(Product)
	product, err := kafka.NewProducer(productConfig)
	if err != nil {
		return nil, err
	}

	return &KafkaClient{
		adminClient: admin,
		producer:    product,
	}, nil
}

func (k *KafkaClient) CreateTopic(topicName string) error {
	_, err := k.adminClient.CreateTopics(
		context.Background(),
		[]kafka.TopicSpecification{{
			Topic:             topicName,
			NumPartitions:     3,
			ReplicationFactor: 3,
		}},
	)
	return err
}

// GetClusterMsg
//
//	@Description: è·å–é›†ç¾¤çš„ä¿¡æ¯
//	@receiver k
//	@return error
func (k *KafkaClient) GetClusterMsg() ([]Message, error) {
	meta, _ := k.adminClient.GetMetadata(nil, true, 5000)
	fmt.Printf("Broker æ•°é‡: %d\n", len(meta.Brokers))

	results := make([]Message, 0)
	for _, b := range meta.Brokers {
		results = append(results, Message{
			ID:   fmt.Sprintf("%d", b.ID),
			Host: b.Host,
			Port: fmt.Sprintf("%d", b.Port),
		})
	}
	for t := range meta.Topics {
		fmt.Printf("Topic: %s\n", t)
	}
	return results, nil
}

// DeleteTopic
//
//	@Description: åˆ é™¤topic
//	@receiver k
//	@param topicName
//	@return error
func (k *KafkaClient) DeleteTopic(topicName string) error {
	ctx, cancel := context.WithTimeout(context.Background(), time.Second*10)
	defer cancel()
	_, err := k.adminClient.DeleteTopics(ctx, []string{topicName}, kafka.SetAdminOperationTimeout(5000))
	if err != nil {
		return err
	}
	return nil
}

// CreateConsumer
//
//	@Description: åˆ›å»ºä¸€ä¸ªkafka consumer
//	@receiver k
//	@param topicNames
//	@param groupID
//	@param offset
//	@return *kafka.Consumer
//	@return error
func (k *KafkaClient) CreateConsumer(topicNames []string, groupID string, consumerID int, offset string) (*kafka.Consumer, error) {
	consumerConfig := getKafkaConfig(Consumer)
	consumerConfig.SetKey("group.id", groupID)
	consumerConfig.SetKey("client.id", fmt.Sprintf("consumer-%d", consumerID))
	consumerConfig.SetKey("auto.offset.reset", offset) //earliestæˆ–latest

	consumer, err := kafka.NewConsumer(consumerConfig)
	if err != nil {
		return nil, err
	}
	err = consumer.SubscribeTopics(topicNames, nil)
	if err != nil {
		return nil, err
	}
	k.consumer = append(k.consumer, consumer)
	return consumer, nil
}

// Close
//
//	@Description: å…³é—­æ‰€æœ‰ç›¸å…³çš„èµ„æºå¯¹è±¡
//	@receiver k
//	@return error
func (k *KafkaClient) Close() error {
	k.producer.Close()
	k.adminClient.Close()
	for _, consumer := range k.consumer {
		consumer.Close()
	}
	return nil
}

// handleDeliveryReports å¤„ç†æ¶ˆæ¯å‘é€çš„ ACK
func handleDeliveryReports(producer *kafka.Producer, tracker *ProducerTracker) {
	for e := range producer.Events() {
		switch ev := e.(type) {
		case *kafka.Message:
			if ev.TopicPartition.Error != nil {
				// å‘é€å¤±è´¥
				atomic.AddInt64(&tracker.failedCount, 1)

				tracker.mu.Lock()
				tracker.failedMsgs = append(tracker.failedMsgs, FailedMessage{
					Value: string(ev.Value),
					Error: ev.TopicPartition.Error,
					Time:  time.Now(),
				})
				tracker.mu.Unlock()

				fmt.Printf("âŒ æ¶ˆæ¯å‘é€å¤±è´¥: %v\n", ev.TopicPartition.Error)
			} else {
				// å‘é€æˆåŠŸ
				atomic.AddInt64(&tracker.successCount, 1)
			}
		case kafka.Error:
			fmt.Printf("âš ï¸ Kafka Error: %v\n", ev)
		}
	}
	fmt.Println("ğŸ“¢ Delivery report handler é€€å‡º")
}

// produceWithBackpressure å¸¦èƒŒå‹æ§åˆ¶çš„æ¶ˆæ¯å‘é€
func produceWithBackpressure(ctx context.Context, producer *kafka.Producer, tracker *ProducerTracker, msg *kafka.Message) error {
	// æ£€æŸ¥å¾…ç¡®è®¤æ¶ˆæ¯æ•°é‡
	for {
		sent := atomic.LoadInt64(&tracker.sentCount)
		success := atomic.LoadInt64(&tracker.successCount)
		failed := atomic.LoadInt64(&tracker.failedCount)
		pending := sent - (success + failed)
		queueLen := int64(producer.Len())

		// èƒŒå‹æ§åˆ¶ï¼šå¦‚æœå¾…ç¡®è®¤æ¶ˆæ¯æ•°è¶…è¿‡é˜ˆå€¼ï¼Œç­‰å¾…
		if pending >= tracker.maxPending || queueLen >= tracker.maxPending {
			atomic.AddInt64(&tracker.backpressureHit, 1)

			// åªåœ¨ç¬¬ä¸€æ¬¡è§¦å‘èƒŒå‹æ—¶æ‰“å°
			hitCount := atomic.LoadInt64(&tracker.backpressureHit)
			if hitCount == 1 || hitCount%100 == 0 {
				fmt.Printf("ğŸ”´ èƒŒå‹è§¦å‘ (ç¬¬ %d æ¬¡): å¾…ç¡®è®¤ %d æ¡ï¼Œé˜Ÿåˆ— %d æ¡ï¼Œç­‰å¾…å¤„ç†...\n",
					hitCount, pending, queueLen)
			}

			select {
			case <-ctx.Done():
				return ctx.Err()
			case <-time.After(10 * time.Millisecond):
				// ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
				continue
			}
		}

		// å¾…ç¡®è®¤æ•°é‡åœ¨é˜ˆå€¼å†…ï¼Œå¯ä»¥å‘é€
		break
	}

	// å‘é€æ¶ˆæ¯
	err := producer.Produce(msg, nil)
	if err != nil {
		return fmt.Errorf("æäº¤æ¶ˆæ¯å¤±è´¥: %w", err)
	}

	// å‘é€æˆåŠŸï¼Œè®¡æ•° +1
	atomic.AddInt64(&tracker.sentCount, 1)
	return nil
}

// waitForCompletion ç­‰å¾…æ‰€æœ‰æ¶ˆæ¯å‘é€å®Œæˆ
func waitForCompletion(ctx context.Context, producer *kafka.Producer, tracker *ProducerTracker) error {
	ticker := time.NewTicker(1 * time.Second)
	defer ticker.Stop()

	startTime := time.Now()
	lastReported := int64(0)

	for {
		sent := atomic.LoadInt64(&tracker.sentCount)
		success := atomic.LoadInt64(&tracker.successCount)
		failed := atomic.LoadInt64(&tracker.failedCount)
		completed := success + failed
		queueLen := producer.Len()

		// æ£€æŸ¥æ˜¯å¦å…¨éƒ¨å®Œæˆ
		if sent > 0 && completed == sent && queueLen == 0 {
			elapsed := time.Since(startTime)
			backpressureHits := atomic.LoadInt64(&tracker.backpressureHit)

			fmt.Printf("\nâœ… å…¨éƒ¨å®Œæˆ! æ€»æ•°: %d, æˆåŠŸ: %d, å¤±è´¥: %d, è€—æ—¶: %v\n",
				sent, success, failed, elapsed)
			fmt.Printf("ğŸ“Š èƒŒå‹ç»Ÿè®¡: è§¦å‘ %d æ¬¡\n", backpressureHits)

			// å¦‚æœæœ‰å¤±è´¥çš„æ¶ˆæ¯ï¼Œæ‰“å°è¯¦æƒ…
			if failed > 0 {
				tracker.mu.Lock()
				fmt.Printf("\nâš ï¸ å¤±è´¥æ¶ˆæ¯è¯¦æƒ… (å…± %d æ¡):\n", len(tracker.failedMsgs))
				for i, msg := range tracker.failedMsgs {
					if i >= 10 {
						fmt.Printf("... è¿˜æœ‰ %d æ¡å¤±è´¥æ¶ˆæ¯\n", len(tracker.failedMsgs)-10)
						break
					}
					fmt.Printf("  %d. æ¶ˆæ¯: %s, é”™è¯¯: %v\n", i+1, msg.Value, msg.Error)
				}
				tracker.mu.Unlock()
			}

			return nil
		}

		select {
		case <-ctx.Done():
			return fmt.Errorf("â±ï¸ è¶…æ—¶: å‘é€ %d æ¡ï¼Œå®Œæˆ %d æ¡ (æˆåŠŸ %d, å¤±è´¥ %d), é˜Ÿåˆ—ä¸­ %d æ¡",
				sent, completed, success, failed, queueLen)

		case <-ticker.C:
			// åªåœ¨è¿›åº¦å˜åŒ–æ—¶æ‰“å°
			if completed != lastReported {
				pending := sent - completed
				progress := float64(completed) / float64(sent) * 100
				elapsed := time.Since(startTime)

				// è®¡ç®—é€Ÿç‡
				rate := float64(completed) / elapsed.Seconds()

				// ä¼°ç®—å‰©ä½™æ—¶é—´
				var eta time.Duration
				if rate > 0 {
					eta = time.Duration(float64(sent-completed)/rate) * time.Second
				}

				backpressureHits := atomic.LoadInt64(&tracker.backpressureHit)

				fmt.Printf("ğŸ“Š è¿›åº¦: %.1f%% (%d/%d) | æˆåŠŸ: %d, å¤±è´¥: %d | å¾…ç¡®è®¤: %d | é˜Ÿåˆ—: %d | èƒŒå‹: %d æ¬¡ | é€Ÿç‡: %.0f msg/s | è€—æ—¶: %v | ETA: %v\n",
					progress, completed, sent, success, failed, pending, queueLen, backpressureHits, rate,
					elapsed.Round(time.Second), eta.Round(time.Second))

				lastReported = completed
			}
		}
	}
}

func createProducer(kafkaOperator *KafkaClient) error {
	// åˆ›å»º trackerï¼Œè®¾ç½®æœ€å¤§å¾…ç¡®è®¤æ¶ˆæ¯æ•°ä¸º 10000
	// å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´è¿™ä¸ªå€¼
	tracker := &ProducerTracker{
		maxPending: 10000, // ğŸ”¥ èƒŒå‹é˜ˆå€¼ï¼šæœ€å¤šå…è®¸ 10000 æ¡æ¶ˆæ¯å¾…ç¡®è®¤
		failedMsgs: make([]FailedMessage, 0),
	}

	// å¯åŠ¨ ACK å¤„ç† goroutine
	go handleDeliveryReports(kafkaOperator.producer, tracker)

	topicName := "this_topic"
	messageCount := 101 // 0 åˆ° 100ï¼Œå…± 101 æ¡

	fmt.Printf("ğŸš€ å¼€å§‹å‘é€ %d æ¡æ¶ˆæ¯ï¼ˆèƒŒå‹é˜ˆå€¼: %dï¼‰...\n", messageCount, tracker.maxPending)
	startTime := time.Now()

	// åˆ›å»ºå¸¦è¶…æ—¶çš„ä¸Šä¸‹æ–‡ï¼ˆç”¨äºèƒŒå‹æ§åˆ¶ï¼‰
	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Minute)
	defer cancel()

	wg := sync.WaitGroup{}

	for i := 0; i <= 100; i++ {
		wg.Add(1)
		go func(i int) {
			defer wg.Done()

			timestamp := time.Now().UnixNano()
			messageValue := fmt.Sprintf("hello world %d", timestamp)

			// ğŸ”¥ ä½¿ç”¨å¸¦èƒŒå‹æ§åˆ¶çš„å‘é€å‡½æ•°
			err := produceWithBackpressure(ctx, kafkaOperator.producer, tracker, &kafka.Message{
				TopicPartition: kafka.TopicPartition{
					Topic:     &topicName,
					Partition: kafka.PartitionAny,
				},
				Value: []byte(messageValue),
			})

			if err != nil {
				fmt.Printf("âš ï¸ å‘é€å¤±è´¥ [msg-%d]: %v\n", i, err)
			}
		}(i)
	}

	// ç­‰å¾…æ‰€æœ‰ goroutine å®Œæˆæ¶ˆæ¯æäº¤
	wg.Wait()

	submitDuration := time.Since(startTime)
	sent := atomic.LoadInt64(&tracker.sentCount)
	backpressureHits := atomic.LoadInt64(&tracker.backpressureHit)

	fmt.Printf("ğŸ“ æ‰€æœ‰æ¶ˆæ¯å·²æäº¤åˆ°é˜Ÿåˆ—ï¼Œå®é™…æäº¤: %d æ¡ï¼Œè€—æ—¶: %vï¼ŒèƒŒå‹è§¦å‘: %d æ¬¡\n",
		sent, submitDuration, backpressureHits)

	// ç­‰å¾…æ‰€æœ‰æ¶ˆæ¯å‘é€å®Œæˆï¼Œæœ€å¤šç­‰å¾… 5 åˆ†é’Ÿ
	fmt.Println("â³ ç­‰å¾…æ‰€æœ‰æ¶ˆæ¯ç¡®è®¤...")
	waitCtx, waitCancel := context.WithTimeout(context.Background(), 5*time.Minute)
	defer waitCancel()

	if err := waitForCompletion(waitCtx, kafkaOperator.producer, tracker); err != nil {
		return fmt.Errorf("å‘é€å¤±è´¥: %w", err)
	}

	totalDuration := time.Since(startTime)
	fmt.Printf("ğŸ“ˆ æ€»è€—æ—¶: %v (æäº¤: %v, ç¡®è®¤: %v)\n",
		totalDuration, submitDuration, totalDuration-submitDuration)

	return nil
}

func createConsumer(kafkaOperator *KafkaClient, consumerId int) {
	consumer, err := kafkaOperator.CreateConsumer([]string{"this_topic"}, "number_one", consumerId, "earliest")
	if err != nil {
		panic(err)
	}

	commitBatchSize := 10
	messageCount := 0
	offsetMap := make(map[kafka.TopicPartition]kafka.Offset)

	for ev := range consumer.Events() { // ä» channel ä¸­è¯»å–äº‹ä»¶
		switch e := ev.(type) {

		case *kafka.Message:
			fmt.Printf("Consumer_%d got msg: %s, partition=%d offset=%d\n",
				consumerId, string(e.Value), e.TopicPartition.Partition, e.TopicPartition.Offset)

			// ä¿å­˜è¯¥åˆ†åŒºæœ€æ–° offset
			tp := kafka.TopicPartition{
				Topic:     e.TopicPartition.Topic,
				Partition: e.TopicPartition.Partition,
			}
			offsetMap[tp] = e.TopicPartition.Offset + 1 // ä¸‹ä¸€æ¡æ¶ˆæ¯offset(Kafkaçš„è¦æ±‚)

			messageCount++

			// è¾¾åˆ°æ‰¹é‡æ•°é‡ï¼Œæäº¤ä¸€æ¬¡
			if messageCount >= commitBatchSize {
				var offsets []kafka.TopicPartition

				for tp, off := range offsetMap {
					tp.Offset = off
					offsets = append(offsets, tp)
				}

				_, err := consumer.CommitOffsets(offsets)
				if err != nil {
					fmt.Println("âŒ æ‰¹é‡æäº¤å¤±è´¥:", err)
				} else {
					fmt.Println("âœ… æ‰¹é‡æäº¤æˆåŠŸ:", offsets)
				}

				// æ¸…ç©ºè®¡æ•°
				messageCount = 0
			}

		// åˆ†åŒºåˆ†é… rebalance
		case kafka.AssignedPartitions:
			fmt.Println("ğŸ“Œ åˆ†é…åˆ†åŒº:", e.Partitions)
			consumer.Assign(e.Partitions)

		case kafka.RevokedPartitions:
			fmt.Println("ğŸ“Œ å›æ”¶åˆ†åŒº")
			consumer.Unassign()

		case kafka.Error:
			fmt.Println("âŒ Kafka Error:", e)
		}
	}
}

func main() {
	kafkaOperator, err := NewKafkaClient()
	if err != nil {
		fmt.Println("âŒ init kafka client error:", err)
		panic(err)
	}
	defer kafkaOperator.Close()

	//if err := kafkaOperator.CreateTopic("this_topic"); err != nil {
	//	fmt.Println("âŒ create topic error:", err)
	//}

	wg := sync.WaitGroup{}
	signalChan := make(chan os.Signal, 1)
	signal.Notify(signalChan, syscall.SIGINT, syscall.SIGTERM)

	// ç”Ÿäº§æ¶ˆæ¯
	wg.Add(1)
	go func() {
		defer wg.Done()
		if err := createProducer(kafkaOperator); err != nil {
			fmt.Printf("âŒ Producer é”™è¯¯: %v\n", err)
		}
	}()

	// æ¶ˆè´¹æ¶ˆæ¯
	for i := 0; i < 3; i++ {
		wg.Add(1)
		go func(i int) {
			defer wg.Done()
			createConsumer(kafkaOperator, i)
		}(i)
	}

	// ç­‰å¾…ä¿¡å·
	<-signalChan
	fmt.Println("\nğŸ›‘ æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
}
